
# 9. Four Ways to Solve Least Squares Problems


1. Pseudo Inverse A⁺
    - works for any matrix
2. Solve AᵀA = Aᵀb
    - works when A has independent columns
3. Orthogonalize first
    - Gram-Schmidt
4. (AᵀA + δ²I)x = Aᵀb

---

## 1. Pseudo Inverse A⁺

- A mxn, A⁺ nxm
    - x in row space is going to column space, it is invertible
        - what does A⁺ is to take vector in column space back to row space.
        - ![](../imgs/MIT_18065_least2_1.png)
    - x in null space is to (0,0), it is not invertible
        - since A⁺ can only take vector in N(Aᵀ) back to (0,0)
    - A⁺A is identity on the top half, and 0 on the bottom half.

- Now I want a simple formular for it.
    - If we were looking for a nice expression, start with the SVD, 
    - if A is invertible, A⁻¹ = V·1/∑·Uᵀ 
    - if A is not invertible, now ∑ is not invertible,  A⁺ = V·∑⁺·Uᵀ
        - ![](../imgs/MIT_18065_least2_2.png)
        - **x = A⁺b** = V·∑⁺·Uᵀ·b


## Solve AᵀA = Aᵀb

- Minimize ‖Ax-b‖² = (Ax-b)ᵀ(Ax-b) = xᵀAᵀAx - 2bᵀAx + bᵀb
    - this is the loss function
- What equation is solved by the best x ?
    - Solution: 
        - if **A has independent columns**, N(A)=0 , then AᵀA is invertible, then
        - AᵀAx̂ = Aᵀb
        - x̂ = (AᵀA)⁻¹Aᵀb
    - most examples, if A is not very big or very difficult, you just create AᵀA and use matlab or octave to solve it quickly.
    - But what if it is difficult to create AᵀA ?  e.g. A has dependent columns.
- What if A has dependent columns ?
    - use A⁺
    - We are going to connect AᵀAx̂ = Aᵀb  to A⁺. 
    - Claim: 
        - A⁺b = (AᵀA)⁻¹Aᵀb  , **when N(A)=0**, i.e. rank=r
        - V∑⁺Uᵀ = (AᵀA)⁻¹Aᵀ

## 3. Orthogonalize first

- the same requirement of N(A)=0.


# 10. Survey of Difficulties with Ax = b

0. x = A⁺b , pseudo inverse
    - an answer for all cases
1. x = A⁻¹b
    - good, normal case 
    - size ok, condition σ₁/σ<sub>n</sub> ok ( ratio of min σ and max σ )
    - octave: A\b
2. m > n = ***rank***
    - AᵀAx̂ = Aᵀb
    - A\b still works
3. `m < n`
    - underdetermined case, don't have enough equations, so I have to put something more in to get a specific answer
    - **typical of deep learning**. 
        - There are so many weights in a deep neural network, the weights will be the unknowns, it wouldn't be linear. 
        - we have many solutions, and we have to pick 1, or we would pick an algorithm to find 1. 
    - So we could pick the minimum norm solution, the shortest solution would be the l2 answer. 
        - Or we could go l1. The big question is that does depp learning and the iteration from stochastic gradient descent go to the minimum l1 ?
4. columns in bad condition, nearly dependent
    - Gram-Schmidt  A = QR
    - we can do Gram-Schmidt ways normally , just like in 18.06. 
        - Or do column pivoting.  You allow yourself to reorder them.
5. Near singular, singular value is very close to 0
    - Inverse problem. You're in dange, its inverse is gonna be unreasonably big.
    - The world of inverse problems thinks of adding a penalty term.
    - min ‖Ax-b‖² + δ²‖x‖²
6. Too big
    - Iterative Method: You never get the exact answer, but you get closer and closer.
7. Way too big
    - giant giant matrix, which is happening these days.
    - randomized linear algebra has popped up. The idea is to use probability to sample the matrix, and work with you samples.





